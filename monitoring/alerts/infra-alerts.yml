groups:
  - name: infrastructure_alerts
    interval: 30s
    rules:
      # Database alerts
      - alert: PostgreSQLDown
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
          team: devops
        annotations:
          summary: "PostgreSQL database is down"
          description: "PostgreSQL has been down for more than 1 minute"
          action: "Check database pod status and logs"

      - alert: PostgreSQLHighConnections
        expr: |
          (
            pg_stat_database_numbackends{datname="shagunintelligence"}
            / pg_settings_max_connections
          ) > 0.8
        for: 5m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "PostgreSQL connection pool nearly exhausted"
          description: "{{ $value | humanizePercentage }} of max connections are in use"

      - alert: PostgreSQLSlowQueries
        expr: |
          rate(pg_stat_statements_mean_time_seconds[5m]) > 1
        for: 10m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "PostgreSQL slow queries detected"
          description: "Average query time is {{ $value }}s"

      - alert: PostgreSQLReplicationLag
        expr: |
          pg_replication_lag_seconds > 10
        for: 5m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "PostgreSQL replication lag high"
          description: "Replication lag is {{ $value }}s"

      # Redis alerts
      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 1m
        labels:
          severity: critical
          team: devops
        annotations:
          summary: "Redis is down"
          description: "Redis has been down for more than 1 minute"

      - alert: RedisHighMemoryUsage
        expr: |
          (
            redis_memory_used_bytes
            / redis_memory_max_bytes
          ) > 0.9
        for: 5m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "Redis memory usage high"
          description: "Redis is using {{ $value | humanizePercentage }} of max memory"

      - alert: RedisRejectedConnections
        expr: |
          increase(redis_rejected_connections_total[5m]) > 5
        for: 5m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "Redis rejecting connections"
          description: "{{ $value }} connections rejected in last 5 minutes"

      # Kubernetes alerts
      - alert: KubernetesPodCrashLooping
        expr: |
          rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: critical
          team: devops
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
          description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last 15 minutes"

      - alert: KubernetesPodNotReady
        expr: |
          sum by (namespace, pod) (
            kube_pod_status_phase{phase=~"Pending|Unknown"} == 1
          ) > 0
        for: 10m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} not ready"
          description: "Pod has been in {{ $labels.phase }} state for more than 10 minutes"

      - alert: KubernetesNodeNotReady
        expr: |
          kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
          team: devops
        annotations:
          summary: "Kubernetes node {{ $labels.node }} not ready"
          description: "Node {{ $labels.node }} has been not ready for 5 minutes"

      - alert: KubernetesPersistentVolumeError
        expr: |
          kube_persistentvolume_status_phase{phase=~"Failed|Pending"} > 0
        for: 5m
        labels:
          severity: critical
          team: devops
        annotations:
          summary: "Persistent volume {{ $labels.persistentvolume }} has error"
          description: "PV is in {{ $labels.phase }} state"

      # Certificate expiry
      - alert: CertificateExpiringSoon
        expr: |
          certmanager_certificate_expiration_timestamp_seconds - time() < 7 * 24 * 60 * 60
        for: 1h
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "Certificate {{ $labels.name }} expiring soon"
          description: "Certificate will expire in {{ $value | humanizeDuration }}"

      # Disk space
      - alert: DiskSpaceLow
        expr: |
          (
            node_filesystem_avail_bytes{mountpoint="/"}
            / node_filesystem_size_bytes{mountpoint="/"}
          ) < 0.1
        for: 5m
        labels:
          severity: critical
          team: devops
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Only {{ $value | humanizePercentage }} disk space remaining"

      # Network issues
      - alert: HighNetworkLatency
        expr: |
          histogram_quantile(0.95,
            rate(http_request_duration_seconds_bucket{job="nginx"}[5m])
          ) > 2
        for: 10m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "High network latency detected"
          description: "95th percentile latency is {{ $value }}s"

      # Load balancer health
      - alert: LoadBalancerUnhealthyBackends
        expr: |
          (
            nginx_upstream_server_unavailable
            / nginx_upstream_server_count
          ) > 0.3
        for: 5m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "Load balancer has unhealthy backends"
          description: "{{ $value | humanizePercentage }} of backends are unhealthy"